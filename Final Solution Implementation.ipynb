{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJbKTZI_W-y5"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/johannfaouzi/pyts.git"
      ],
      "id": "wJbKTZI_W-y5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O52XBg5YejTv"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_addons"
      ],
      "id": "O52XBg5YejTv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8f5ecb7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import gc\n",
        "import sys\n",
        "import random\n",
        "import json\n",
        "import random, string\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler,LabelEncoder\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from scipy import signal\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import KFold,StratifiedGroupKFold,StratifiedKFold\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.signal import savgol_filter\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fftpack import fft, dct\n",
        "import tensorflow\n",
        "from tensorflow.keras.callbacks import *\n",
        "import scipy\n",
        "from scipy.stats.mstats import gmean\n",
        "from scipy.stats import chi2\n",
        "from matplotlib import patches\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm  \n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Conv1D, AveragePooling1D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import ReLU\n",
        "import tensorflow_addons as tfa\n",
        "from keras.layers import Conv1D,Conv2D, Reshape, GaussianNoise\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "f8f5ecb7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9b0c439"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=1903):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything(seed=42)"
      ],
      "id": "f9b0c439"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3c1e8e5"
      },
      "outputs": [],
      "source": [
        "maindir = \".\" # Directory with your files\n",
        "traincsv = maindir+\"/Update_train.csv\"\n",
        "testcsv = maindir+\"/Updated_Test.csv\"\n",
        "sample_sub = maindir+'/Updated_Sample_Submission.csv'\n",
        "wav_csv = maindir+\"/Zindi_Contest_Spectral_Cholesterol.csv\"\n",
        "\n",
        "cholesterol_signature = maindir + \"/Zindi Contest Spectra - Cholesterol.csv\"\n",
        "hemoglobin_signature = maindir + \"/Zindi Contest Spectra - Hemoglobin.csv\"\n",
        "glucose_signature = maindir + \"/Zindi Contest Spectra - Glucose.csv\"\n",
        "skin_fat_blood_signature = maindir + \"/Zindi Contest Spectra - skin_fat_blood.csv\"\n",
        "\n",
        "wave_lengths = pd.read_csv(wav_csv)\n",
        "\n",
        "targets = [\n",
        "           'hdl_cholesterol_human', \n",
        "           'cholesterol_ldl_human', \n",
        "           'hemoglobin(hgb)_human'\n",
        "           ]"
      ],
      "id": "c3c1e8e5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bcYrGEkZHGA"
      },
      "outputs": [],
      "source": [
        "wave_len_info  = wave_lengths['Cholesterol'].values[1:].tolist()\n",
        "len(wave_len_info)"
      ],
      "id": "_bcYrGEkZHGA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKEU-ADXhLWs"
      },
      "outputs": [],
      "source": [
        "waves = []\n",
        "\n",
        "for x in range(148):\n",
        "  waves.append(float(wave_len_info[x]))\n",
        "\n",
        "waves = np.array(waves, dtype = float)\n",
        "waves.shape"
      ],
      "id": "pKEU-ADXhLWs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx-zUIUSorAt"
      },
      "outputs": [],
      "source": [
        "waves_sig = []\n",
        "\n",
        "for x in range(170):\n",
        "  waves_sig.append(float(wave_len_info[x]))\n",
        "\n",
        "waves_sig = np.array(waves_sig, dtype = float)\n",
        "waves_sig.shape"
      ],
      "id": "fx-zUIUSorAt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpoBNG5so1wV"
      },
      "outputs": [],
      "source": [
        "mask_wls_sg = (waves_sig >= 950) & (waves_sig <= 1350)\n",
        "clipped_waves_sig = waves_sig[mask_wls_sg]\n",
        "clipped_waves_sig.shape"
      ],
      "id": "xpoBNG5so1wV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_U_fIvJ908qc"
      },
      "outputs": [],
      "source": [
        "mask_wls = (waves >= 950) & (waves <= 1350)\n",
        "clipped_waves = waves[mask_wls]\n",
        "clipped_waves.shape"
      ],
      "id": "_U_fIvJ908qc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EI9Yd_Gf1Qbo"
      },
      "outputs": [],
      "source": [
        "waves_clipped = clipped_waves.copy()\n",
        "cols = [str(c) for c in clipped_waves]"
      ],
      "id": "EI9Yd_Gf1Qbo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMzaFx9IR8gP"
      },
      "outputs": [],
      "source": [
        "# Loading datasets\n",
        "train = pd.read_csv(traincsv)\n",
        "test = pd.read_csv(testcsv)"
      ],
      "id": "IMzaFx9IR8gP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IrXa_zdxCFz"
      },
      "outputs": [],
      "source": [
        "train_abs = train[[f'absorbance{i}' for i in range(148)]]\n",
        "train_abs.columns = wave_len_info[:148]\n",
        "\n",
        "test_abs = test[[f'absorbance{i}' for i in range(148)]]\n",
        "test_abs.columns = wave_len_info[:148]"
      ],
      "id": "2IrXa_zdxCFz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91MDXo_VxTOG"
      },
      "outputs": [],
      "source": [
        "test.drop([f'absorbance{i}' for i in range(148)], axis=1,  inplace=True)\n",
        "train.drop([f'absorbance{i}' for i in range(148)], axis=1, inplace=True)"
      ],
      "id": "91MDXo_VxTOG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BWQ8RIRxXB0"
      },
      "outputs": [],
      "source": [
        "train = pd.concat([train_abs, train], axis=1)\n",
        "test =  pd.concat([test_abs,  test], axis=1)"
      ],
      "id": "-BWQ8RIRxXB0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzzQ90z73k_u"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ],
      "id": "yzzQ90z73k_u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f6gLdkSOC4l"
      },
      "outputs": [],
      "source": [
        "cholesterol_signature_df = pd.read_csv(cholesterol_signature)\n",
        "hemoglobin_signature_df = pd.read_csv(hemoglobin_signature)\n",
        "glucose_signature_df = pd.read_csv(glucose_signature)\n",
        "skin_fat_blood_signature_df = pd.read_csv(skin_fat_blood_signature)"
      ],
      "id": "8f6gLdkSOC4l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JT_hDstRd3Y"
      },
      "outputs": [],
      "source": [
        "hemoglobin_signature_df"
      ],
      "id": "7JT_hDstRd3Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIYtgxmgO0xP"
      },
      "outputs": [],
      "source": [
        "cholesterol_signature_df.rename({'Cholesterol' : 'wavelength', 'Unnamed: 1':'absorbance_coef'},axis=1, inplace=True)\n",
        "hemoglobin_signature_df.rename({'deox_hb' : 'wavelength', 'Unnamed: 1' :'deoxy_absorbance_coef'},axis=1, inplace=True)\n",
        "hemoglobin_signature_df.rename({'oxy_hb' : 'wavelength', 'Unnamed: 4' :'oxy_absorbance_coef'},axis=1, inplace=True)\n",
        "glucose_signature_df.rename({'Glucose' : 'wavelength', 'Unnamed: 1':'absorbance_coef'},axis=1, inplace=True)\n",
        "skin_fat_blood_signature_df.rename({'Skin' : 'skin_wavelength', 'Unnamed: 1':'skin_absorbance_coef'},axis=1, inplace=True)\n",
        "skin_fat_blood_signature_df.rename({'Fat' : 'fat_wavelength', 'Unnamed: 4':'fat_absorbance_coef'},axis=1, inplace=True)\n",
        "skin_fat_blood_signature_df.rename({'Deoxy' : 'deoxy_wavelength', 'Unnamed: 7':'deoxy_absorbance_coef'},axis=1, inplace=True)"
      ],
      "id": "rIYtgxmgO0xP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za2cOly1nxNB"
      },
      "outputs": [],
      "source": [
        "hemoglobin_signature_df.head()"
      ],
      "id": "za2cOly1nxNB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZdfGI_zn1Jk"
      },
      "outputs": [],
      "source": [
        "cholesterol_signature_df.drop(cholesterol_signature_df.index[0], inplace=True)\n",
        "hemoglobin_signature_df.drop(hemoglobin_signature_df.index[0], inplace=True)\n",
        "glucose_signature_df.drop(glucose_signature_df.index[0],inplace=True)\n",
        "skin_fat_blood_signature_df.drop(skin_fat_blood_signature_df.index[0], inplace=True)"
      ],
      "id": "7ZdfGI_zn1Jk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAgpCLgjQQsW"
      },
      "outputs": [],
      "source": [
        "cholesterol_sig = cholesterol_signature_df['absorbance_coef'].values[mask_wls_sg]\n",
        "deoxy_hemoglobin_sig =  hemoglobin_signature_df['deoxy_absorbance_coef'].values[mask_wls]\n",
        "oxy_hemoglobin_sig =  hemoglobin_signature_df['oxy_absorbance_coef'].values[mask_wls]\n",
        "glucose_sig    = glucose_signature_df['absorbance_coef'].values[mask_wls_sg]\n",
        "skin_sig = skin_fat_blood_signature_df['skin_absorbance_coef'].values[mask_wls_sg]\n",
        "fat_sig = skin_fat_blood_signature_df['fat_absorbance_coef'].values[mask_wls_sg]\n",
        "deoxy_sig = skin_fat_blood_signature_df['deoxy_absorbance_coef'].values[mask_wls_sg]"
      ],
      "id": "gAgpCLgjQQsW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLeVEK-URYqb"
      },
      "outputs": [],
      "source": [
        "cholesterol_sig.shape, deoxy_hemoglobin_sig.shape, oxy_hemoglobin_sig.shape, glucose_sig.shape, skin_sig.shape, skin_sig.shape, fat_sig.shape, deoxy_sig.shape"
      ],
      "id": "KLeVEK-URYqb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHcEzGZ9NhCC"
      },
      "outputs": [],
      "source": [
        "cholesterol_signature_df"
      ],
      "id": "iHcEzGZ9NhCC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq3-4LfTEBgw"
      },
      "outputs": [],
      "source": [
        "train = train[train.donation_id != 6824].reset_index(drop=True) # Remove Donation_ID 6824"
      ],
      "id": "Gq3-4LfTEBgw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmBH5CWGttJ-"
      },
      "outputs": [],
      "source": [
        "train_spectra = train[cols].values\n",
        "test_spectra = test[cols].values"
      ],
      "id": "WmBH5CWGttJ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bqfF9dLV6at"
      },
      "outputs": [],
      "source": [
        "cholesterol_sig = np.array(cholesterol_sig, dtype = float)\n",
        "deoxy_hemoglobin_sig = np.array(deoxy_hemoglobin_sig, dtype = float)\n",
        "oxy_hemoglobin_sig = np.array(oxy_hemoglobin_sig, dtype = float)\n",
        "glucose_sig    = np.array(glucose_sig, dtype = float)\n",
        "skin_sig = np.array(skin_sig, dtype = float)\n",
        "fat_sig = np.array(fat_sig, dtype = float)\n",
        "deoxy_sig = np.array(deoxy_sig, dtype = float)"
      ],
      "id": "6bqfF9dLV6at"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhEIP2ny8Goh"
      },
      "outputs": [],
      "source": [
        "num_plots = 1\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(clipped_waves, train_spectra[x, :], 'r')"
      ],
      "id": "zhEIP2ny8Goh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oFgy5gkD5E5"
      },
      "outputs": [],
      "source": [
        "from pandas.compat.numpy import np\n",
        "from scipy import newaxis as nA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, cohen_kappa_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, f1_score \n",
        "\n",
        "\n",
        "# Important Utils\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_loss(history):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    for l in loss_list:\n",
        "            plt.plot(epochs, history.history[l], 'b', label='Training loss (' + \n",
        "                     str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "\n",
        "class GlobalStandardScaler(object):\n",
        "    \"\"\"Scales to unit standard deviation and mean centering using global mean and std of X, skleran like API\"\"\"\n",
        "    def __init__(self,with_mean=True, with_std=True, normfact=1.0):\n",
        "        self._with_mean = with_mean\n",
        "        self._with_std = with_std\n",
        "        self.std = None\n",
        "        self.normfact=normfact\n",
        "        self.mean = None\n",
        "        self._fitted = False\n",
        "        \n",
        "    def fit(self,X, y = None):\n",
        "        X = np.array(X)\n",
        "        self.mean = X.mean()\n",
        "        self.std = X.std()\n",
        "        self._fitted = True\n",
        "        \n",
        "    def transform(self,X, y=None):\n",
        "        if self._fitted:\n",
        "            X = np.array(X)\n",
        "            if self._with_mean:\n",
        "                X=X-self.mean\n",
        "            if self._with_std:\n",
        "                X=X/(self.std*self.normfact)\n",
        "            return X\n",
        "        else:\n",
        "            print(\"Scaler is not fitted\")\n",
        "            return\n",
        "            \n",
        "    def inverse_transform(self,X, y=None):\n",
        "        if self._fitted:\n",
        "            X = np.array(X)\n",
        "            if self._with_std:\n",
        "                X=X*self.std*self.normfact\n",
        "            if self._with_mean:\n",
        "                X=X+self.mean\n",
        "            return X\n",
        "        else:\n",
        "            print(\"Scaler is not fitted\")\n",
        "            return\n",
        "            \n",
        "    def fit_transform(self,X, y=None):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "\n",
        "class EmscScaler(object):\n",
        "    def __init__(self,order=1):\n",
        "        self.order = order\n",
        "        self._mx = None\n",
        "        \n",
        "    def mlr(self,x,y):\n",
        "        \"\"\"Multiple linear regression fit of the columns of matrix x \n",
        "        (dependent variables) to constituent vector y (independent variables)\n",
        "        \n",
        "        order -     order of a smoothing polynomial, which can be included \n",
        "                    in the set of independent variables. If order is\n",
        "                    not specified, no background will be included.\n",
        "        b -         fit coeffs\n",
        "        f -         fit result (m x 1 column vector)\n",
        "        r -         residual   (m x 1 column vector)\n",
        "        \"\"\"\n",
        "        \n",
        "        if self.order > 0:\n",
        "            s= np.ones((len(y),1))\n",
        "            for j in range(self.order):\n",
        "                s= np.concatenate((s,(np.arange(0,1+(1.0/(len(y)-1)),1.0/(len(y)-1))**j)[:,nA]),1)\n",
        "            X= np.concatenate((x, s),1)\n",
        "        else:\n",
        "            X = x\n",
        "        \n",
        "        #calc fit b=fit coefficients\n",
        "        b = np.dot(np.dot(np.linalg.pinv(np.dot(np.transpose(X),X)),np.transpose(X)),y)\n",
        "        f = np.dot(X,b)\n",
        "        r = y - f\n",
        "\n",
        "        return b,f,r\n",
        "\n",
        "    \n",
        "    def inverse_transform(self, X, y=None):\n",
        "        print(\"Warning: inverse transform not possible with Emsc\")\n",
        "        return X\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"fit to X (get average spectrum), y is a passthrough for pipeline compatibility\"\"\"\n",
        "        self._mx = np.mean(X,axis=0)[:,nA]\n",
        "        \n",
        "    def transform(self, X, y=None, copy=None):\n",
        "        if type(self._mx) == type(None):\n",
        "            print(\"EMSC not fit yet. run .fit method on reference spectra\")\n",
        "        else:\n",
        "            #do fitting\n",
        "            corr = np.zeros(X.shape)\n",
        "            for i in range(len(X)):\n",
        "                b,f,r = self.mlr(self._mx, X[i,:][:,nA])\n",
        "                corr[i,:] = np.reshape((r/b[0,0]) + self._mx, (corr.shape[1],))\n",
        "            return corr\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "          "
      ],
      "id": "5oFgy5gkD5E5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niNs0taLDRQW"
      },
      "outputs": [],
      "source": [
        "def transform_c_ldl(row):\n",
        "    return str(row[\"donation_id\"]) + \"_cholesterol_ldl_human\" +  \"-\" + row[\"cholesterol_ldl_human\"]\n",
        "\n",
        "def transform_c_hdl(row):\n",
        "    return str(row[\"donation_id\"]) + \"_hdl_cholesterol_human\" +  \"-\" + row[\"hdl_cholesterol_human\"]\n",
        "\n",
        "def transform_hemo(row):\n",
        "    return str(row[\"donation_id\"]) + \"_hemoglobin(hgb)_human\" +  \"-\" + row[\"hemoglobin(hgb)_human\"]"
      ],
      "id": "niNs0taLDRQW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eChOw-SYFdZk"
      },
      "outputs": [],
      "source": [
        "test['donation_id'] = test['donation_id'].apply(lambda x : str(x).split('_')[-1])"
      ],
      "id": "eChOw-SYFdZk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOLwd4Hbl1q0"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([train, test], ignore_index = True)\n",
        "data"
      ],
      "id": "EOLwd4Hbl1q0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-7DZ4VRFq2G"
      },
      "outputs": [],
      "source": [
        "drop_waves = [str(c) for c in waves if c not in waves_clipped]\n",
        "drop_waves[:5]"
      ],
      "id": "f-7DZ4VRFq2G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPq1vpXg4VSd"
      },
      "outputs": [],
      "source": [
        "del drop_waves[0]"
      ],
      "id": "WPq1vpXg4VSd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BctU-n9I4aqJ"
      },
      "outputs": [],
      "source": [
        "drop_waves.append('900')"
      ],
      "id": "BctU-n9I4aqJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg_S8-j4Fgmf"
      },
      "outputs": [],
      "source": [
        "data.drop(columns = drop_waves, axis = 1, inplace=True)"
      ],
      "id": "Rg_S8-j4Fgmf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEobJ3b83HXR"
      },
      "outputs": [],
      "source": [
        "test_ = data[data.Reading_ID.isin(test.Reading_ID.values.tolist())].reset_index(drop=True)\n",
        "train_ = data[~data.Reading_ID.isin(test.Reading_ID.values.tolist())].reset_index(drop=True)"
      ],
      "id": "VEobJ3b83HXR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdFCu7oi3aYD"
      },
      "outputs": [],
      "source": [
        "train_.shape, test_.shape"
      ],
      "id": "UdFCu7oi3aYD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yfOO0p-g8zn"
      },
      "outputs": [],
      "source": [
        "train_spectra = train_[cols].values\n",
        "test_spectra  = test_[cols].values"
      ],
      "id": "5yfOO0p-g8zn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2BRMg4SiSEA"
      },
      "outputs": [],
      "source": [
        "train_[cols].head()"
      ],
      "id": "z2BRMg4SiSEA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI-dkpMLukP8"
      },
      "outputs": [],
      "source": [
        "feature_cols_env = ['humidity', 'temperature']"
      ],
      "id": "eI-dkpMLukP8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlHH_zHdEvN7"
      },
      "outputs": [],
      "source": [
        "train_[cols + feature_cols_env] = train_.groupby([\"donation_id\"])[cols + feature_cols_env].transform(\"median\")\n",
        "test_[cols + feature_cols_env]  = test_.groupby([\"donation_id\"])[cols + feature_cols_env].transform(\"median\")"
      ],
      "id": "MlHH_zHdEvN7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WheNPwZEYat"
      },
      "outputs": [],
      "source": [
        "train_ = train_.drop_duplicates(subset=['donation_id'], keep= 'first')\n",
        "test_  = test_.drop_duplicates(subset=['donation_id'],  keep= 'first')"
      ],
      "id": "5WheNPwZEYat"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxfbGOPMQWMh"
      },
      "outputs": [],
      "source": [
        "train_.reset_index(drop=True, inplace=True)\n",
        "test_.reset_index(drop=True, inplace=True)"
      ],
      "id": "vxfbGOPMQWMh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL_WvMbt682g"
      },
      "outputs": [],
      "source": [
        "X_train = train_[cols].values\n",
        "X_test = test_[cols].values"
      ],
      "id": "aL_WvMbt682g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20-DGyna50td"
      },
      "outputs": [],
      "source": [
        "train_"
      ],
      "id": "20-DGyna50td"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIXB_g4bN8pH"
      },
      "outputs": [],
      "source": [
        "cholesterol_sig.shape"
      ],
      "id": "qIXB_g4bN8pH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2fruVzLhOE7"
      },
      "outputs": [],
      "source": [
        "num_plots = 1\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, cholesterol_sig, 'r')"
      ],
      "id": "y2fruVzLhOE7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uO3H3eqLB5-"
      },
      "outputs": [],
      "source": [
        "train_spec = train_[cols].values\n",
        "test_spec = test_[cols].values"
      ],
      "id": "-uO3H3eqLB5-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6R1ZfKaRg1m"
      },
      "outputs": [],
      "source": [
        "train_.drop([c for c in train_.columns if \"absorbance\" in c], axis = 1, inplace=True)"
      ],
      "id": "Q6R1ZfKaRg1m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rREJ3TMPqaCD"
      },
      "outputs": [],
      "source": [
        "test_.drop([c for c in test_.columns if \"absorbance\" in c], axis = 1, inplace=True)"
      ],
      "id": "rREJ3TMPqaCD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpkve3Ipq3Dx"
      },
      "outputs": [],
      "source": [
        "df_hdl = train_.drop([targets[1], targets[2]], axis=1)\n",
        "df_ldl = train_.drop([targets[0], targets[2]], axis=1)\n",
        "df_hgb = train_.drop([targets[0], targets[1]], axis=1)"
      ],
      "id": "wpkve3Ipq3Dx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix_902pprPSz"
      },
      "outputs": [],
      "source": [
        "test_.drop(targets , axis=1, inplace=True)"
      ],
      "id": "ix_902pprPSz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOwu5lKgsC1W"
      },
      "outputs": [],
      "source": [
        "df_hgb['hemoglobin(hgb)_human'].value_counts()"
      ],
      "id": "xOwu5lKgsC1W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpIQ9CP2s7-M"
      },
      "outputs": [],
      "source": [
        "df_ldl"
      ],
      "id": "lpIQ9CP2s7-M"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw1DtTShsX18"
      },
      "outputs": [],
      "source": [
        "df_hdl_f = df_hdl.copy()\n",
        "df_ldl_f = df_ldl.copy()\n",
        "df_hgb_f = df_hgb.copy() "
      ],
      "id": "Hw1DtTShsX18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf_-kl1qswrS"
      },
      "outputs": [],
      "source": [
        "df_hdl_f"
      ],
      "id": "Cf_-kl1qswrS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RIPKzPbD3UN"
      },
      "outputs": [],
      "source": [
        "df_test_hdl = test_.copy()\n",
        "df_test_ldl = test_.copy()\n",
        "df_test_hgb = test_.copy()"
      ],
      "id": "1RIPKzPbD3UN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbCPT4l0xQNi"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, df_hdl_f[cols].values[x, :], 'r')"
      ],
      "id": "XbCPT4l0xQNi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jgOpHYUx8B4"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, fat_sig, 'r')"
      ],
      "id": "1jgOpHYUx8B4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcWb6mkTyT1g"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, cholesterol_sig, 'r')"
      ],
      "id": "YcWb6mkTyT1g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5lP2eVKxz_9"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, deoxy_hemoglobin_sig, 'r')"
      ],
      "id": "M5lP2eVKxz_9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkgIJh2OVv6o"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, oxy_hemoglobin_sig, 'r')"
      ],
      "id": "YkgIJh2OVv6o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alXcl_f0Esm-"
      },
      "outputs": [],
      "source": [
        "df_hgb_f"
      ],
      "id": "alXcl_f0Esm-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfbubCz4E5-H"
      },
      "outputs": [],
      "source": [
        "cols[:5]"
      ],
      "id": "TfbubCz4E5-H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKzP_2q4ftn6"
      },
      "outputs": [],
      "source": [
        "df_hdl_f.drop([c for c in df_hdl_f.columns if c not in cols + ['id', 'std','donation_id', 'temperature', 'humidity']+ targets], inplace=True, axis=1)\n",
        "df_ldl_f.drop([c for c in df_ldl_f.columns if c not in cols + ['id', 'std','donation_id', 'temperature', 'humidity']+ targets],inplace=True, axis=1)\n",
        "df_hgb_f.drop([c for c in df_hgb_f.columns if c not in cols + ['id', 'std','donation_id', 'temperature', 'humidity']+ targets],inplace=True, axis=1)"
      ],
      "id": "wKzP_2q4ftn6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3uCZ1zXEcon"
      },
      "outputs": [],
      "source": [
        "cholesterol_sig.min(), cholesterol_sig.max(), cholesterol_sig.shape"
      ],
      "id": "r3uCZ1zXEcon"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF3qkpjoEiyS"
      },
      "outputs": [],
      "source": [
        "df_ldl_f[cols].values[0, :].min(),  df_ldl_f[cols].values[0, :].max()"
      ],
      "id": "MF3qkpjoEiyS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I-t4JafLsIl"
      },
      "outputs": [],
      "source": [
        "def compute_correlation(whole, sub):\n",
        "\n",
        "  whole_mean = whole.mean()\n",
        "\n",
        "  whole = whole - whole_mean\n",
        "  sub   = sub - whole_mean\n",
        "\n",
        "  corr = signal.correlate(whole, sub, mode = 'same')\n",
        "\n",
        "  return corr"
      ],
      "id": "8I-t4JafLsIl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpHIQVFAhRwy"
      },
      "outputs": [],
      "source": [
        "df_hdl_f.shape"
      ],
      "id": "tpHIQVFAhRwy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcgWxsPIUJuH"
      },
      "source": [
        "**USE CORRELATION TO ISOLATE PURE COMPOUND PATTERNS FROM THE READINGS**"
      ],
      "id": "hcgWxsPIUJuH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajy216mqHV6T"
      },
      "outputs": [],
      "source": [
        "df_hdl_f_arr = np.array(df_hdl_f[cols].values)\n",
        "test_hdl_arr = np.array(df_test_hdl[cols].values)\n",
        "df_ldl_f_arr = np.array(df_ldl_f[cols].values)\n",
        "df_hgb_f_arr = np.array(df_hgb_f[cols].values)"
      ],
      "id": "ajy216mqHV6T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMfo6dcIZXYz"
      },
      "outputs": [],
      "source": [
        "df_hdl_f_arr.shape, df_ldl_f.shape, df_hgb_f.shape"
      ],
      "id": "kMfo6dcIZXYz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yo7elMHMftG"
      },
      "outputs": [],
      "source": [
        "hb_corr_arr = []\n",
        "\n",
        "for i in range(df_hdl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hdl_f_arr[i, :].squeeze(), deoxy_hemoglobin_sig)\n",
        "  hb_corr_arr.append(arr)\n",
        "\n",
        "hdl_deoxy  = np.array(hb_corr_arr)"
      ],
      "id": "5yo7elMHMftG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsVD5LVNbVZ1"
      },
      "outputs": [],
      "source": [
        "hb_corr_arr = []\n",
        "\n",
        "for i in range(df_ldl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_ldl_f_arr[i, :].squeeze(), deoxy_hemoglobin_sig)\n",
        "  hb_corr_arr.append(arr)\n",
        "\n",
        "ldl_deoxy  = np.array(hb_corr_arr)"
      ],
      "id": "GsVD5LVNbVZ1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkkF_RDscUMo"
      },
      "outputs": [],
      "source": [
        "hb_corr_arr = []\n",
        "\n",
        "for i in range(df_hgb_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hgb_f_arr[i, :].squeeze(), deoxy_hemoglobin_sig)\n",
        "  hb_corr_arr.append(arr)\n",
        "\n",
        "hgb_deoxy  = np.array(hb_corr_arr)"
      ],
      "id": "lkkF_RDscUMo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iQpFL0qdgVK"
      },
      "outputs": [],
      "source": [
        "hb_corr_arr = []\n",
        "\n",
        "for i in range(test_hdl_arr.shape[0]):\n",
        "  arr = compute_correlation(test_hdl_arr[i, :].squeeze(), deoxy_hemoglobin_sig)\n",
        "  hb_corr_arr.append(arr)\n",
        "\n",
        "\n",
        "test_deoxy  = np.array(hb_corr_arr)"
      ],
      "id": "7iQpFL0qdgVK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7LrHZLfaGjX"
      },
      "outputs": [],
      "source": [
        "oxy_corr_arr = []\n",
        "\n",
        "for i in range(df_hdl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hdl_f_arr[i, :].squeeze(), oxy_hemoglobin_sig)\n",
        "  oxy_corr_arr.append(arr)\n",
        "\n",
        "hdl_oxy  = np.array(oxy_corr_arr)"
      ],
      "id": "g7LrHZLfaGjX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoAj-5dpbfkq"
      },
      "outputs": [],
      "source": [
        "oxy_corr_arr = []\n",
        "\n",
        "for i in range(df_ldl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_ldl_f_arr[i, :].squeeze(), oxy_hemoglobin_sig)\n",
        "  oxy_corr_arr.append(arr)\n",
        "\n",
        "ldl_oxy  = np.array(oxy_corr_arr)"
      ],
      "id": "CoAj-5dpbfkq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGuQ1ksccJYx"
      },
      "outputs": [],
      "source": [
        "oxy_corr_arr = []\n",
        "\n",
        "for i in range(df_hgb_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hgb_f_arr[i, :].squeeze(), oxy_hemoglobin_sig)\n",
        "  oxy_corr_arr.append(arr)\n",
        "\n",
        "hgb_oxy  = np.array(oxy_corr_arr)"
      ],
      "id": "fGuQ1ksccJYx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slO74dEvdwnS"
      },
      "outputs": [],
      "source": [
        "oxy_corr_arr = []\n",
        "\n",
        "for i in range(test_hdl_arr.shape[0]):\n",
        "  arr = compute_correlation(test_hdl_arr[i, :].squeeze(), oxy_hemoglobin_sig)\n",
        "  oxy_corr_arr.append(arr)\n",
        "\n",
        "test_oxy  = np.array(oxy_corr_arr)"
      ],
      "id": "slO74dEvdwnS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBbeT7u5aa_j"
      },
      "outputs": [],
      "source": [
        "fat_corr_arr = []\n",
        "\n",
        "for i in range(df_hdl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hdl_f_arr[i, :].squeeze(), fat_sig)\n",
        "  fat_corr_arr.append(arr)\n",
        "\n",
        "hdl_fat  = np.array(fat_corr_arr)"
      ],
      "id": "VBbeT7u5aa_j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voNWUtgTbtR-"
      },
      "outputs": [],
      "source": [
        "fat_corr_arr = []\n",
        "\n",
        "for i in range(df_ldl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_ldl_f_arr[i, :].squeeze(), fat_sig)\n",
        "  fat_corr_arr.append(arr)\n",
        "\n",
        "ldl_fat  = np.array(fat_corr_arr)"
      ],
      "id": "voNWUtgTbtR-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_gFiIqHcDYD"
      },
      "outputs": [],
      "source": [
        "fat_corr_arr = []\n",
        "\n",
        "for i in range(df_hgb_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hgb_f_arr[i, :].squeeze(), fat_sig)\n",
        "  fat_corr_arr.append(arr)\n",
        "\n",
        "hgb_fat  = np.array(fat_corr_arr)"
      ],
      "id": "Y_gFiIqHcDYD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCv6jHxwd86Z"
      },
      "outputs": [],
      "source": [
        "fat_corr_arr = []\n",
        "\n",
        "for i in range(test_hdl_arr.shape[0]):\n",
        "  arr = compute_correlation(test_hdl_arr[i, :].squeeze(), fat_sig)\n",
        "  fat_corr_arr.append(arr)\n",
        "\n",
        "test_fat  = np.array(fat_corr_arr)"
      ],
      "id": "nCv6jHxwd86Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSjsyEwAaqsr"
      },
      "outputs": [],
      "source": [
        "glu_corr_arr = []\n",
        "\n",
        "for i in range(df_hdl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hdl_f_arr[i, :].squeeze(), glucose_sig)\n",
        "  glu_corr_arr.append(arr)\n",
        "\n",
        "hdl_glu  = np.array(glu_corr_arr)"
      ],
      "id": "wSjsyEwAaqsr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m02K55S-b1VS"
      },
      "outputs": [],
      "source": [
        "glu_corr_arr = []\n",
        "\n",
        "for i in range(df_ldl_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_ldl_f_arr[i, :].squeeze(), glucose_sig)\n",
        "  glu_corr_arr.append(arr)\n",
        "\n",
        "ldl_glu  = np.array(glu_corr_arr)"
      ],
      "id": "m02K55S-b1VS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tHxvc1Ib6l9"
      },
      "outputs": [],
      "source": [
        "glu_corr_arr = []\n",
        "\n",
        "for i in range(df_hgb_f_arr.shape[0]):\n",
        "  arr = compute_correlation(df_hgb_f_arr[i, :].squeeze(), glucose_sig)\n",
        "  glu_corr_arr.append(arr)\n",
        "\n",
        "hgb_glu  = np.array(glu_corr_arr)"
      ],
      "id": "0tHxvc1Ib6l9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXYXWz4ReEoP"
      },
      "outputs": [],
      "source": [
        "glu_corr_arr = []\n",
        "\n",
        "for i in range(test_hdl_arr.shape[0]):\n",
        "  arr = compute_correlation(test_hdl_arr[i, :].squeeze(), glucose_sig)\n",
        "  glu_corr_arr.append(arr)\n",
        "\n",
        "test_glu  = np.array(glu_corr_arr)"
      ],
      "id": "fXYXWz4ReEoP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AJxiirLbB9r"
      },
      "outputs": [],
      "source": [
        "net_hdl_f_arr = hdl_deoxy - (hdl_fat + hdl_glu)"
      ],
      "id": "_AJxiirLbB9r"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hlwC6Zucl3n"
      },
      "outputs": [],
      "source": [
        "net_ldl_f_arr = ldl_deoxy - ldl_glu"
      ],
      "id": "9hlwC6Zucl3n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDEn8bWfdAVt"
      },
      "outputs": [],
      "source": [
        "net_hgb_f_arr = hgb_deoxy  - (hgb_fat + hgb_glu)"
      ],
      "id": "nDEn8bWfdAVt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nsSD7I9bNsp"
      },
      "outputs": [],
      "source": [
        "df_hdl_f[cols] = net_hdl_f_arr"
      ],
      "id": "6nsSD7I9bNsp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDAiymrlcscJ"
      },
      "outputs": [],
      "source": [
        "df_ldl_f[cols] = net_ldl_f_arr"
      ],
      "id": "lDAiymrlcscJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JzbcWnedXqj"
      },
      "outputs": [],
      "source": [
        "df_hgb_f[cols] = net_hgb_f_arr"
      ],
      "id": "5JzbcWnedXqj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haE4fvfDeNC3"
      },
      "outputs": [],
      "source": [
        "net_test_f_arr = test_deoxy - (test_fat + test_glu)"
      ],
      "id": "haE4fvfDeNC3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB5KG1BYnb3Z"
      },
      "outputs": [],
      "source": [
        "net_test_ldl = test_deoxy - test_glu"
      ],
      "id": "VB5KG1BYnb3Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9wvuIpEnrrE"
      },
      "outputs": [],
      "source": [
        "net_test_hdl =  net_test_f_arr"
      ],
      "id": "-9wvuIpEnrrE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7_35_h1eo1S"
      },
      "outputs": [],
      "source": [
        "df_test_hdl[cols] = net_test_hdl \n",
        "df_test_hgb[cols] = net_test_f_arr\n",
        "df_test_ldl[cols] = net_test_ldl "
      ],
      "id": "n7_35_h1eo1S"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFxlIkpcWnO1"
      },
      "source": [
        "**Spectral Preprocessing**"
      ],
      "id": "DFxlIkpcWnO1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpUbJH30wveI"
      },
      "outputs": [],
      "source": [
        "df_hdl_f['hdl_cholesterol_human'] = df_hdl_f['hdl_cholesterol_human'].map({'low': 0, 'ok': 1, 'high' : 2})\n",
        "df_hgb_f['hemoglobin(hgb)_human'] = df_hgb_f['hemoglobin(hgb)_human'].map({'low': 0, 'ok': 1, 'high' : 2})\n",
        "df_ldl_f['cholesterol_ldl_human'] = df_ldl_f['cholesterol_ldl_human'].map({'low': 0, 'ok': 1, 'high' : 2})"
      ],
      "id": "fpUbJH30wveI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKJKN3-wSbqH"
      },
      "outputs": [],
      "source": [
        "g_scaler1 = GlobalStandardScaler()\n",
        "emsc_corr1 = EmscScaler()"
      ],
      "id": "IKJKN3-wSbqH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB9Qkgfa061w"
      },
      "outputs": [],
      "source": [
        "hgb_tgt = df_hgb_f['hemoglobin(hgb)_human'].values\n",
        "hdl_tgt = df_hdl_f['hdl_cholesterol_human'].values\n",
        "ldl_tgt = df_ldl_f['cholesterol_ldl_human'].values"
      ],
      "id": "KB9Qkgfa061w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k4mIG_xDqCq"
      },
      "outputs": [],
      "source": [
        "hgb_spec = df_hgb_f[cols].values\n",
        "hdl_spec = df_hdl_f[cols].values\n",
        "ldl_spec = df_ldl_f[cols].values"
      ],
      "id": "-k4mIG_xDqCq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mriXneZ8Sa2d"
      },
      "outputs": [],
      "source": [
        "hgb_scaled = g_scaler1.fit_transform(hgb_spec, hgb_tgt)\n",
        "hgb_corrected = emsc_corr1.fit_transform(hgb_scaled,hgb_tgt)\n",
        "df_hgb_f[cols] = hgb_corrected"
      ],
      "id": "mriXneZ8Sa2d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54DP1HDP2KfJ"
      },
      "outputs": [],
      "source": [
        "g_scaler2 = GlobalStandardScaler()\n",
        "emsc_corr2 = EmscScaler()\n",
        "\n",
        "hdl_scaled = g_scaler2.fit_transform(hdl_spec, hdl_tgt)\n",
        "hdl_corrected = emsc_corr2.fit_transform(hdl_scaled, hdl_tgt)\n",
        "df_hdl_f[cols] = hdl_corrected"
      ],
      "id": "54DP1HDP2KfJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVs2PkLt2Sw1"
      },
      "outputs": [],
      "source": [
        "g_scaler3 = GlobalStandardScaler()\n",
        "emsc_corr3 = EmscScaler()\n",
        "\n",
        "ldl_scaled = g_scaler3.fit_transform(ldl_spec, ldl_tgt)\n",
        "ldl_corrected = emsc_corr3.fit_transform(ldl_scaled, ldl_tgt)\n",
        "df_ldl_f[cols] = ldl_corrected"
      ],
      "id": "IVs2PkLt2Sw1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFXVS_jI1ThH"
      },
      "outputs": [],
      "source": [
        "hgb_env = df_hgb_f[feature_cols_env].values\n",
        "hdl_env = df_hdl_f[feature_cols_env].values\n",
        "ldl_env = df_ldl_f[feature_cols_env].values"
      ],
      "id": "tFXVS_jI1ThH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFVGbiQ6tFwz"
      },
      "outputs": [],
      "source": [
        "df_ldl_f"
      ],
      "id": "OFVGbiQ6tFwz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7AqOhu_xkyT"
      },
      "outputs": [],
      "source": [
        "hgb_smoothed = savgol_filter(df_hgb_f[cols], window_length=5, polyorder=2, deriv=1, axis = 0)\n",
        "df_hgb_f[cols] = hgb_smoothed"
      ],
      "id": "z7AqOhu_xkyT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efF_IWoCV4kP"
      },
      "outputs": [],
      "source": [
        "hdl_smoothed = savgol_filter(df_hdl_f[cols], window_length=5, polyorder=2, deriv=1, axis = 0)\n",
        "df_hdl_f[cols] = hdl_smoothed"
      ],
      "id": "efF_IWoCV4kP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4pgVCRUWTDn"
      },
      "outputs": [],
      "source": [
        "ldl_smoothed = savgol_filter(df_ldl_f[cols], window_length=5, polyorder=2, deriv=1, axis = 0)\n",
        "df_ldl_f[cols] = ldl_smoothed"
      ],
      "id": "T4pgVCRUWTDn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLzZ55Y7Fx7n"
      },
      "outputs": [],
      "source": [
        "hgb_spec_t = df_test_hgb[cols].values\n",
        "hdl_spec_t = df_test_hdl[cols].values\n",
        "ldl_spec_t = df_test_ldl[cols].values"
      ],
      "id": "pLzZ55Y7Fx7n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxKgQJdC377t"
      },
      "outputs": [],
      "source": [
        "test_scaled = g_scaler1.transform(hgb_spec_t)\n",
        "test_corrected = emsc_corr1.transform(test_scaled)\n",
        "df_test_hgb[cols] = test_corrected"
      ],
      "id": "OxKgQJdC377t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ulLQEGVGNSE"
      },
      "outputs": [],
      "source": [
        "test_scaled = g_scaler2.transform(hdl_spec_t)\n",
        "test_corrected = emsc_corr2.transform(test_scaled)\n",
        "df_test_hdl[cols] = test_corrected"
      ],
      "id": "1ulLQEGVGNSE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhbCbGRPGQYO"
      },
      "outputs": [],
      "source": [
        "test_scaled = g_scaler3.transform(ldl_spec_t)\n",
        "test_corrected = emsc_corr3.transform(test_scaled)\n",
        "df_test_ldl[cols] = test_corrected"
      ],
      "id": "fhbCbGRPGQYO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFo6XM_ppLxE"
      },
      "outputs": [],
      "source": [
        "test_smoothed = savgol_filter(df_test_hdl[cols], window_length=5, polyorder=2, deriv=1, axis = 0)\n",
        "df_test_hdl[cols] = test_smoothed"
      ],
      "id": "TFo6XM_ppLxE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdWD3q8RUwkf"
      },
      "outputs": [],
      "source": [
        "test_smoothed = savgol_filter(df_test_ldl[cols], window_length=5, polyorder=2, deriv=1, axis = 0)\n",
        "df_test_ldl[cols] = test_smoothed"
      ],
      "id": "QdWD3q8RUwkf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N26piKQiUywf"
      },
      "outputs": [],
      "source": [
        "test_smoothed = savgol_filter(df_test_hgb[cols], window_length=5, polyorder=2, deriv=1, axis = 0)\n",
        "df_test_hgb[cols] = test_smoothed"
      ],
      "id": "N26piKQiUywf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RsSDvIDSvCN"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, hgb_smoothed[x, :], 'r')"
      ],
      "id": "6RsSDvIDSvCN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXz4EhKcWntC"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, ldl_smoothed[x, :], 'b')"
      ],
      "id": "dXz4EhKcWntC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hlQ77zgVShK"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, hdl_smoothed[x, :], 'b')"
      ],
      "id": "6hlQ77zgVShK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1VQXi0euPmv"
      },
      "outputs": [],
      "source": [
        "num_plots = 15\n",
        "\n",
        "for x in range(num_plots):\n",
        "  plt.plot(waves_clipped, test_smoothed[x, :], 'b')"
      ],
      "id": "o1VQXi0euPmv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzePdkoW8S69"
      },
      "outputs": [],
      "source": [
        "_ = gc.collect()"
      ],
      "id": "qzePdkoW8S69"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-i2cw-17PSN"
      },
      "outputs": [],
      "source": [
        "len(cols)"
      ],
      "id": "n-i2cw-17PSN"
    },
    {
      "cell_type": "code",
      "source": [
        "df_hdl_f['sg_std'] = df_hdl_f[cols].apply(lambda x : np.std(x.values), axis=1)\n",
        "df_ldl_f['sg_std'] = df_ldl_f[cols].apply(lambda x : np.std(x.values), axis=1)\n",
        "df_hgb_f['sg_std'] = df_hgb_f[cols].apply(lambda x : np.std(x.values), axis=1)"
      ],
      "metadata": {
        "id": "GaRARF2VmB_p"
      },
      "id": "GaRARF2VmB_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_hdl['sg_std'] = df_test_hdl[cols].apply(lambda x : np.std(x.values), axis=1)\n",
        "df_test_ldl['sg_std'] = df_test_ldl[cols].apply(lambda x : np.std(x.values), axis=1)\n",
        "df_test_hgb['sg_std'] = df_test_hgb[cols].apply(lambda x : np.std(x.values), axis=1)"
      ],
      "metadata": {
        "id": "LOHK3QsCmC6Q"
      },
      "id": "LOHK3QsCmC6Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3m_RViLYSF3"
      },
      "outputs": [],
      "source": [
        "df_hdl_f"
      ],
      "id": "t3m_RViLYSF3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10jHrUW6VUzH"
      },
      "outputs": [],
      "source": [
        "feature_cols = cols\n",
        "feature_cols_env = ['temperature' , 'humidity', 'sg_std']"
      ],
      "id": "10jHrUW6VUzH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RQTcOrnia_s"
      },
      "outputs": [],
      "source": [
        "new_data_train = pd.concat([df_hdl_f[cols], df_ldl_f[cols], df_hgb_f[cols]], ignore_index=True)\n",
        "new_data_test  = pd.concat([df_test_hdl[cols], df_test_ldl[cols], df_test_hgb[cols]], ignore_index=True)"
      ],
      "id": "5RQTcOrnia_s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg1gs0C-izSw"
      },
      "outputs": [],
      "source": [
        "all_data = pd.concat([new_data_train, new_data_test], ignore_index=True)\n",
        "all_data.head()"
      ],
      "id": "lg1gs0C-izSw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFLUR_bLiYmG"
      },
      "outputs": [],
      "source": [
        "# Finding optimal no. of clusters\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "clusters = range(1,10)\n",
        "meanDistortions  = []\n",
        "\n",
        "for k in clusters:\n",
        "    model=KMeans(n_clusters=k)\n",
        "    model.fit(all_data)\n",
        "    prediction=model.predict(all_data)\n",
        "    meanDistortions.append(sum(np.min(cdist(all_data, model.cluster_centers_, 'euclidean'), axis=1)) / all_data.shape[0])\n",
        "\n",
        "\n",
        "plt.plot(clusters, meanDistortions, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Average distortion')\n",
        "plt.title('Selecting k with the Elbow Method')"
      ],
      "id": "nFLUR_bLiYmG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz2BBX00VutE"
      },
      "outputs": [],
      "source": [
        "def fe_cluster(train, test, features, n_clusters= 2, SEED=42):\n",
        "\n",
        "    def create_cluster(train, test, features, n_clusters=n_clusters):\n",
        "\n",
        "        train = train.fillna(0)\n",
        "        test =  test.fillna(0)\n",
        "        \n",
        "        train_ = train[features].copy()\n",
        "        test_ = test[features].copy()\n",
        "\n",
        "        kmeans = MiniBatchKMeans(n_clusters=n_clusters, max_iter=300, batch_size=32, random_state= SEED)\n",
        "                \n",
        "        kmeans.fit(pd.concat((train_, test_), axis=0).reset_index(drop=True))\n",
        "\n",
        "        train['clusters'] = kmeans.predict(train_.values)\n",
        "        test['clusters']  = kmeans.predict(test_.values)\n",
        "\n",
        "        return train, test\n",
        "\n",
        "    train, test = create_cluster(\n",
        "        train, test, features, n_clusters=n_clusters)\n",
        "    \n",
        "    return train, test"
      ],
      "id": "oz2BBX00VutE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u-nxFiBV569"
      },
      "outputs": [],
      "source": [
        "df_ldl_f, df_test_ldl = fe_cluster(df_ldl_f, df_test_ldl, n_clusters = 2, features = feature_cols)"
      ],
      "id": "4u-nxFiBV569"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ngrKT9wJb-R"
      },
      "outputs": [],
      "source": [
        "df_hdl_f, df_test_hdl = fe_cluster(df_hdl_f, df_test_hdl, n_clusters =  2, features = feature_cols)"
      ],
      "id": "9ngrKT9wJb-R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2GTsPguJlnb"
      },
      "outputs": [],
      "source": [
        "df_hgb_f, df_test_hgb = fe_cluster(df_hgb_f, df_test_hgb, n_clusters = 2, features = feature_cols)"
      ],
      "id": "S2GTsPguJlnb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy79-Xd3jt6l"
      },
      "outputs": [],
      "source": [
        "metrics.fowlkes_mallows_score(df_hdl_f['hdl_cholesterol_human'],df_hdl_f['clusters']) # evaluate hdl_cholesterol_human clusters"
      ],
      "id": "cy79-Xd3jt6l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bse57_cvlMEn"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import zscore"
      ],
      "id": "bse57_cvlMEn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsuwwkeokt6d"
      },
      "outputs": [],
      "source": [
        "df_hdl_f[cols] = df_hdl_f[cols].apply(zscore)\n",
        "df_hdl_f[cols] = df_hdl_f[cols].apply(zscore)\n",
        "df_hdl_f[cols] = df_hdl_f[cols].apply(zscore)"
      ],
      "id": "wsuwwkeokt6d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojQ4ucHUlWJB"
      },
      "outputs": [],
      "source": [
        "for x in cols:\n",
        "\n",
        "  for df in [df_hdl_f, df_ldl_f, df_hgb_f]:\n",
        "\n",
        "    indexNames_larger = df[df[x]>3].index\n",
        "    indexNames_lesser = df[df[x]<-3].index\n",
        "    # Delete these row indexes from dataFrame\n",
        "    \n",
        "    df.drop(indexNames_larger , inplace=True)\n",
        "    df.drop(indexNames_lesser , inplace=True)"
      ],
      "id": "ojQ4ucHUlWJB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogw9HwsVl6-k"
      },
      "outputs": [],
      "source": [
        "df_hdl_f.shape, df_ldl_f.shape, df_hgb_f.shape"
      ],
      "id": "ogw9HwsVl6-k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AyK-BHdpAbc"
      },
      "outputs": [],
      "source": [
        "df_hdl.shape, df_ldl.shape, df_hgb.shape"
      ],
      "id": "7AyK-BHdpAbc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvJKapFVzwis"
      },
      "outputs": [],
      "source": [
        "df_hdl_f.reset_index(drop=True, inplace=True)"
      ],
      "id": "DvJKapFVzwis"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpMiEJnwWNNf"
      },
      "outputs": [],
      "source": [
        "x = df_hgb_f.copy()\n",
        "t = df_test_hgb.copy()\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(x[cols])\n",
        "T_pca = pca.transform(t[cols])\n",
        "\n",
        "pca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
        "\n",
        "X_pca = pd.DataFrame(X_pca, columns=pca_cols, index=df_hgb_f.index)\n",
        "T_pca = pd.DataFrame(T_pca, columns=pca_cols, index=df_test_hgb.index)\n",
        "\n",
        "df_hgb_f = pd.concat([df_hgb_f, X_pca], axis=1)\n",
        "df_test_hgb = pd.concat([df_test_hgb, T_pca], axis=1)"
      ],
      "id": "UpMiEJnwWNNf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy4Se_7QpNx0"
      },
      "outputs": [],
      "source": [
        "df_hgb_f"
      ],
      "id": "jy4Se_7QpNx0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaZKQG2AkzZx"
      },
      "outputs": [],
      "source": [
        "# PCA Viz\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.scatterplot(data=df_hgb_f, x=\"PC1\", y=\"PC2\", hue='hemoglobin(hgb)_human', alpha=0.8, palette=\"deep\")\n",
        "plt.show()"
      ],
      "id": "VaZKQG2AkzZx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTHxYu-RWL5J"
      },
      "outputs": [],
      "source": [
        "x = df_hdl_f.copy()\n",
        "t = df_test_hdl.copy()\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(x[cols])\n",
        "T_pca = pca.transform(t[cols])\n",
        "\n",
        "pca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
        "\n",
        "X_pca = pd.DataFrame(X_pca, columns=pca_cols, index=df_hdl_f.index)\n",
        "T_pca = pd.DataFrame(T_pca, columns=pca_cols, index=df_test_hdl.index)\n",
        "\n",
        "df_hdl_f = pd.concat([df_hdl_f, X_pca], axis=1)\n",
        "df_test_hdl = pd.concat([df_test_hdl, T_pca], axis=1)\n"
      ],
      "id": "bTHxYu-RWL5J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_DOfPQxk8HS"
      },
      "outputs": [],
      "source": [
        "# PCA Viz\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.scatterplot(data=df_hdl_f, x=\"PC1\", y=\"PC2\", hue='hdl_cholesterol_human', alpha=0.8, palette=\"deep\")\n",
        "plt.show()"
      ],
      "id": "v_DOfPQxk8HS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiCo5FrVSMFg"
      },
      "outputs": [],
      "source": [
        "x = df_ldl_f.copy()\n",
        "t = df_test_ldl.copy()\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "X_pca = pca.fit_transform(x[cols])\n",
        "T_pca = pca.transform(t[cols])\n",
        "\n",
        "pca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
        "\n",
        "X_pca = pd.DataFrame(X_pca, columns=pca_cols, index=df_ldl_f.index)\n",
        "T_pca = pd.DataFrame(T_pca, columns=pca_cols, index=df_test_ldl.index)\n",
        "\n",
        "df_ldl_f = pd.concat([df_ldl_f, X_pca], axis=1)\n",
        "df_test_ldl = pd.concat([df_test_ldl, T_pca], axis=1)"
      ],
      "id": "UiCo5FrVSMFg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql7WcwhGlDec"
      },
      "outputs": [],
      "source": [
        "# PCA Viz\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.scatterplot(data=df_ldl_f, x=\"PC1\", y=\"PC2\", hue='cholesterol_ldl_human', alpha=0.8, palette=\"deep\")\n",
        "plt.show()"
      ],
      "id": "Ql7WcwhGlDec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ARxDqLmWZ5O"
      },
      "outputs": [],
      "source": [
        "del pca\n",
        "_  = gc.collect()"
      ],
      "id": "9ARxDqLmWZ5O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uP7DR2bWd1N"
      },
      "outputs": [],
      "source": [
        "feature_cols_env = ['humidity', 'temperature', 'clusters', 'sg_std'] + pca_cols"
      ],
      "id": "2uP7DR2bWd1N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-MuDLErXRFW"
      },
      "outputs": [],
      "source": [
        "len(feature_cols_env)"
      ],
      "id": "g-MuDLErXRFW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtHCoAV-gKjB"
      },
      "outputs": [],
      "source": [
        "df_hdl_f.shape, df_ldl_f.shape, df_hgb_f.shape"
      ],
      "id": "xtHCoAV-gKjB"
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols_env.remove('clusters')"
      ],
      "metadata": {
        "id": "P5Qwfhwp9K3k"
      },
      "id": "P5Qwfhwp9K3k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixg-3d0Fmavh"
      },
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "sc.fit(pd.concat([\n",
        "                  df_hdl_f[feature_cols_env],\n",
        "                  df_ldl_f[feature_cols_env],\n",
        "                  df_hgb_f[feature_cols_env],\n",
        "                  df_test_hdl[feature_cols_env],\n",
        "                  df_test_ldl[feature_cols_env],\n",
        "                  df_test_hgb[feature_cols_env]\n",
        "], ignore_index=True))\n",
        "\n",
        "df_hdl_f[feature_cols_env] = sc.transform(df_hdl_f[feature_cols_env])\n",
        "df_ldl_f[feature_cols_env] = sc.transform(df_ldl_f[feature_cols_env])\n",
        "df_hgb_f[feature_cols_env] = sc.transform(df_hgb_f[feature_cols_env])\n",
        "\n",
        "df_test_hdl[feature_cols_env] = sc.transform(df_test_hdl[feature_cols_env])\n",
        "df_test_ldl[feature_cols_env] = sc.transform(df_test_ldl[feature_cols_env])\n",
        "df_test_hgb[feature_cols_env] = sc.transform(df_test_hgb[feature_cols_env])"
      ],
      "id": "ixg-3d0Fmavh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJQUnT-1qwwu"
      },
      "outputs": [],
      "source": [
        "df_hdl_f = pd.get_dummies(df_hdl_f, prefix='cluster', columns = ['clusters'])\n",
        "df_ldl_f = pd.get_dummies(df_ldl_f, prefix='cluster', columns = ['clusters'])\n",
        "df_hgb_f = pd.get_dummies(df_hgb_f, prefix='cluster', columns = ['clusters'])"
      ],
      "id": "xJQUnT-1qwwu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY5liFRl3T2f"
      },
      "outputs": [],
      "source": [
        "df_test_hdl = pd.get_dummies(df_test_hdl, prefix='cluster', columns = ['clusters'])\n",
        "df_test_ldl = pd.get_dummies(df_test_ldl, prefix='cluster', columns = ['clusters'])\n",
        "df_test_hgb = pd.get_dummies(df_test_hgb, prefix='cluster', columns = ['clusters'])"
      ],
      "id": "PY5liFRl3T2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0GjnrV572Po"
      },
      "outputs": [],
      "source": [
        "df_test_ldl"
      ],
      "id": "E0GjnrV572Po"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwJA99D5CC1m"
      },
      "outputs": [],
      "source": [
        "df_hdl_f.shape"
      ],
      "id": "NwJA99D5CC1m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18e3n9Nj8JgK"
      },
      "outputs": [],
      "source": [
        "feature_cols_env = ['temperature',\t'humidity',\t'PC1',\t'PC2',\t'cluster_0',\t'cluster_1', 'sg_std']"
      ],
      "id": "18e3n9Nj8JgK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5dgFBDVYAle"
      },
      "source": [
        "# **Model Training Utils**"
      ],
      "id": "g5dgFBDVYAle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwvHiTvPKQ0v"
      },
      "outputs": [],
      "source": [
        "# 2D CNN Model Training Util\n",
        "def train_model_NN(data, test_data, n_folds):\n",
        "\n",
        "  preds = []\n",
        "\n",
        "  df = data.copy()\n",
        "\n",
        "  oof_acc = []\n",
        "\n",
        "  folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state = 42)\n",
        "\n",
        "  for fold_ , (trn_idx, val_idx) in enumerate(folds.split(df, df[tgt])):\n",
        "\n",
        "    print('Fold:',fold_)\n",
        "\n",
        "    if tgt == 'hdl_cholesterol_human':\n",
        "\n",
        "  \n",
        "      train_data = df.loc[trn_idx].reset_index(drop=True)\n",
        "      val_data   = df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "      x_train_p, Y_train = train_data[feature_cols].copy(), train_data[tgt].values\n",
        "\n",
        "      x_test_p, Y_test = val_data[feature_cols].copy(), val_data[tgt].values\n",
        "\n",
        "      x_train_s, x_test_s = train_data[feature_cols_env].copy(), val_data[feature_cols_env].copy()\n",
        "\n",
        "      x_test_data, x_test_aux = test_data[feature_cols].copy(), test_data[feature_cols_env].copy()\n",
        "\n",
        "      y_train_ = Y_train.copy()\n",
        "\n",
        "      y_train_ = to_categorical(y_train_ , num_classes = len(targets))\n",
        "\n",
        "      X_train_f = x_train_p.copy() \n",
        "\n",
        "      x_train_imgs = make_image(X_train_f.values)\n",
        "      x_valid_imgs = make_image(x_test_p.values)\n",
        "      x_test_imgs = make_image(x_test_data.values)\n",
        "\n",
        "\n",
        "    elif tgt == 'cholesterol_ldl_human':\n",
        "\n",
        "\n",
        "      train_data = df.loc[trn_idx].reset_index(drop=True)\n",
        "      val_data   = df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "      x_train_p, Y_train = train_data[feature_cols].copy(), train_data[tgt].values\n",
        "\n",
        "      x_test_p, Y_test = val_data[feature_cols].copy(), val_data[tgt].values\n",
        "\n",
        "      x_train_s, x_test_s = train_data[feature_cols_env].copy(), val_data[feature_cols_env].copy()\n",
        "\n",
        "      x_test_data, x_test_aux = test_data[feature_cols].copy(), test_data[feature_cols_env].copy()\n",
        "\n",
        "\n",
        "      y_train_ = Y_train.copy()\n",
        "\n",
        "      y_train_ = to_categorical(y_train_ , num_classes = len(targets))\n",
        "\n",
        "      X_train_f = x_train_p.copy() \n",
        "\n",
        "      x_train_imgs = make_image(X_train_f.values)\n",
        "      x_valid_imgs = make_image(x_test_p.values)\n",
        "      x_test_imgs = make_image(x_test_data.values)\n",
        "\n",
        "    else:\n",
        "\n",
        "      \n",
        "      train_data = df.loc[trn_idx].reset_index(drop=True)\n",
        "      val_data   = df.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "      x_train_p, Y_train = train_data[feature_cols].copy(), train_data[tgt].values\n",
        "\n",
        "      x_test_p, Y_test = val_data[feature_cols].copy(), val_data[tgt].values\n",
        "\n",
        "      x_train_s, x_test_s = train_data[feature_cols_env].copy(), val_data[feature_cols_env].copy()\n",
        "\n",
        "      x_test_data, x_test_aux = test_data[feature_cols].copy(), test_data[feature_cols_env].copy()\n",
        "\n",
        "      y_train_ = Y_train.copy()\n",
        "\n",
        "      y_train_ = to_categorical(y_train_ , num_classes = len(targets))\n",
        "\n",
        "      X_train_f = x_train_p.copy()  \n",
        "\n",
        "      x_train_imgs = make_image(X_train_f.values)\n",
        "      x_valid_imgs = make_image(x_test_p.values)\n",
        "      x_test_imgs = make_image(x_test_data.values)\n",
        "\n",
        "    y_test_  = Y_test.copy()\n",
        "\n",
        "    y_test_  = to_categorical(y_test_,  num_classes = len(targets))\n",
        "\n",
        "    X_valid_f = x_valid_imgs.copy() \n",
        "    X_test_f  = x_test_imgs.copy()\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices(((x_train_imgs, x_train_s.values), y_train_))\n",
        "    train_dataset = train_dataset.shuffle(len(y_train_))  \n",
        "    train_dataset = train_dataset.batch(128).cache()\n",
        "    train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices(((X_valid_f, x_test_s.values), y_test_))\n",
        "    valid_dataset = valid_dataset.batch(128).cache()\n",
        "    valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "    del x_train_imgs, x_valid_imgs\n",
        "    _ =gc.collect()\n",
        "\n",
        "\n",
        "    model = get_spectra_vgg(n_wavenumbers= X_valid_f.shape[1],\n",
        "                            n_features=3, compiled=True)\n",
        "\n",
        "\n",
        "    cb_es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                            patience= 20, mode=\"min\", \n",
        "                                            restore_best_weights=True, verbose=1)\n",
        "    \n",
        "    cb_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, \n",
        "                                                patience=10, mode=\"min\",\n",
        "                                                min_lr=0.00005, verbose=1)\n",
        "\n",
        "    \n",
        "    history = model.fit(train_dataset, \n",
        "                        validation_data =valid_dataset, \n",
        "                        callbacks = [cb_es, cb_lr],\n",
        "                        epochs = 1000) \n",
        "\n",
        "\n",
        "\n",
        "    plot_loss(history)\n",
        "\n",
        "    del history\n",
        "    _ = gc.collect()\n",
        "\n",
        "    y_pre = model.predict([X_valid_f, x_test_s.values])\n",
        "\n",
        "    y_pred = np.argmax(y_pre, axis=-1)\n",
        "\n",
        "    score = accuracy_score(Y_test, y_pred) \n",
        "\n",
        "    oof_acc.append(score)\n",
        "\n",
        "    tt_preds = model.predict([x_test_imgs, x_test_aux.values])\n",
        "\n",
        "    preds.append(tt_preds)\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "    plt.scatter(Y_test, y_pred)\n",
        "    plt.show()\n",
        "    print(\"OOF Accuracy: \",score)\n",
        "\n",
        "  print(f\"{tgt} mean accuracy {np.mean(oof_acc)}\")\n",
        "  \n",
        "  return preds"
      ],
      "id": "VwvHiTvPKQ0v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itn99BDHM6b6"
      },
      "outputs": [],
      "source": [
        "targets"
      ],
      "id": "itn99BDHM6b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkM1RC18mKol"
      },
      "outputs": [],
      "source": [
        "# ref - https://github.com/devitrylouis/imaging_time_series\n",
        "\n",
        "def tabulate(x, y, f):\n",
        "    \"\"\"Return a table of f(x, y). Useful for the Gram-like operations.\"\"\"\n",
        "    return np.vectorize(f)(*np.meshgrid(x, y, sparse=True))\n",
        "\n",
        "def cos_sum(a, b):\n",
        "    \"\"\"To work with tabulate.\"\"\"\n",
        "    return(math.cos(a+b))\n",
        "\n",
        "def sin_diff(a, b):\n",
        "    \"\"\"To work with tabulate.\"\"\"\n",
        "    return(math.sin(a-b))\n",
        "\n",
        "def create_time_serie(size, time):\n",
        "    \"\"\"Generate a time serie of length size and dynamic with respect to time.\"\"\"\n",
        "    # Generating time-series\n",
        "    support = np.arange(0, size)\n",
        "    serie = np.cos(support + float(time))\n",
        "    return(t, serie)"
      ],
      "id": "xkM1RC18mKol"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba2mqYz1NZee"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# ref - https://github.com/devitrylouis/imaging_time_series\n",
        "\n",
        "class GASF:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def transform(self, serie):\n",
        "        \"\"\"Compute the Gramian Angular Field of an image\"\"\"\n",
        "        # Min-Max scaling\n",
        "        min_ = np.amin(serie)\n",
        "        max_ = np.amax(serie)\n",
        "        scaled_serie = (2*serie - max_ - min_)/(max_ - min_)\n",
        "\n",
        "        # Floating point inaccuracy!\n",
        "        scaled_serie = np.where(scaled_serie >= 1., 1., scaled_serie)\n",
        "        scaled_serie = np.where(scaled_serie <= -1., -1., scaled_serie)\n",
        "\n",
        "        # Polar encoding\n",
        "        phi = np.arccos(scaled_serie)\n",
        "        # Note! The computation of r is not necessary\n",
        "        r = np.linspace(0, 1, len(scaled_serie))\n",
        "\n",
        "        # GAF Computation (every term of the matrix)\n",
        "        gaf = tabulate(phi, phi, cos_sum)\n",
        "\n",
        "        return(gaf, phi, r, scaled_serie)\n",
        "    \n",
        "class GADF:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def transform(self, serie):\n",
        "        \"\"\"Compute the Gramian Angular Field of an image\"\"\"\n",
        "        # Min-Max scaling\n",
        "        min_ = np.amin(serie)\n",
        "        max_ = np.amax(serie)\n",
        "        scaled_serie = (2*serie - max_ - min_)/(max_ - min_)\n",
        "\n",
        "        # Floating point inaccuracy!\n",
        "        scaled_serie = np.where(scaled_serie >= 1., 1., scaled_serie)\n",
        "        scaled_serie = np.where(scaled_serie <= -1., -1., scaled_serie)\n",
        "\n",
        "        # Polar encoding\n",
        "        phi = np.arccos(scaled_serie)\n",
        "        # Note! The computation of r is not necessary\n",
        "        r = np.linspace(0, 1, len(scaled_serie))\n",
        "\n",
        "        # GAF Computation (every term of the matrix)\n",
        "        gaf = tabulate(phi, phi, sin_diff)\n",
        "\n",
        "        return(gaf, phi, r, scaled_serie)"
      ],
      "id": "ba2mqYz1NZee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm5vH6trmOwp"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *\n",
        "\n",
        "def get_spectra_vgg(n_wavenumbers, n_features=1, compiled=True):\n",
        "\n",
        "    filter_size = 3\n",
        "    filters_cnt = 32\n",
        "    pooling = AveragePooling2D\n",
        "\n",
        "    inp = Input(shape=(n_wavenumbers, n_wavenumbers, n_features))\n",
        "    aux_inp = Input(shape=(len(feature_cols_env),))\n",
        "\n",
        "    conv1 = Conv2D(filters_cnt*2, filter_size, activation='softmax')(inp)\n",
        "    bn1 = Dropout(0.2)(conv1)\n",
        "\n",
        "    conv2 = Conv2D(filters_cnt*4, filter_size, activation='softmax')(bn1)\n",
        "    bn2 = Dropout(0.2)(conv2)\n",
        "\n",
        "    pool1 = MaxPool2D(pool_size=(2,2))(bn2)\n",
        "\n",
        "    pool_n = pool1\n",
        "    filter_mult = 2\n",
        "    n_blocks = 3\n",
        "    for i in range(n_blocks):\n",
        "        conv_n = Conv2D(filters_cnt * filter_mult, filter_size, activation='softmax')(pool_n)\n",
        "        bn_n = Dropout(0.2)(conv_n)\n",
        "\n",
        "        conv_n2 = Conv2D(filters_cnt * filter_mult, filter_size, activation='softmax')(bn_n)\n",
        "        bn_n2 = Dropout(0.2)(conv_n2)\n",
        "\n",
        "        pool_n = MaxPool2D(pool_size=(2,2))(bn_n2)\n",
        "        filter_mult *= 2\n",
        "\n",
        "    x2 = Dense(32, activation='relu')(aux_inp)\n",
        "    x2 = Dense(64, activation='relu')(x2)\n",
        "\n",
        "    output = Flatten()(pool_n)\n",
        "\n",
        "    cat = tf.keras.layers.Concatenate()([x2, output])\n",
        "\n",
        "    dropout = Dropout(0.25)(cat)\n",
        "    out = Dense(3 , activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=[inp, aux_inp], outputs=out)\n",
        "\n",
        "    if compiled:\n",
        "              \n",
        "        model.compile(loss= 'categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=[tf.keras.metrics.Recall(name='recall'),\n",
        "                               tf.keras.metrics.Precision(name='precision'), \n",
        "                               'accuracy'] )\n",
        "        \n",
        "    return model"
      ],
      "id": "Cm5vH6trmOwp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Signal-to-2d-image Utils**"
      ],
      "metadata": {
        "id": "IQ-CYSMu_A9m"
      },
      "id": "IQ-CYSMu_A9m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bnDyWyngOnq"
      },
      "outputs": [],
      "source": [
        "from pyts.image import MarkovTransitionField\n",
        "from pyts.datasets import load_gunpoint\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def make_gadf_image(x_arr):\n",
        "\n",
        "  counter = 0\n",
        "  gadf = GADF()\n",
        "\n",
        "  x_train_gadf_images = np.zeros((x_arr.shape[0],x_arr.shape[1],x_arr.shape[1]))\n",
        "  for i in x_arr:\n",
        "    img = gadf.transform(i)\n",
        "    x_train_gadf_images[counter] = img[0]\n",
        "\n",
        "    counter = counter + 1\n",
        "    \n",
        "  return x_train_gadf_images\n",
        "\n",
        "def make_gasf_image(x_arr):\n",
        "\n",
        "  gasf = GASF()\n",
        "\n",
        "  counter = 0\n",
        "\n",
        "  x_train_gasf_images = np.zeros((x_arr.shape[0],x_arr.shape[1],x_arr.shape[1]))\n",
        "  for i in x_arr:\n",
        "    img = gasf.transform(i)\n",
        "    x_train_gasf_images[counter] = img[0]\n",
        "\n",
        "    counter = counter + 1\n",
        "    \n",
        "  return x_train_gasf_images\n",
        "\n",
        "def apply_markov_transition(x_arr):\n",
        "\n",
        "  mtf = MarkovTransitionField(image_size=x_arr.shape[-1])\n",
        "  x_train_mtf_images = np.zeros((x_arr.shape[0],x_arr.shape[-1],x_arr.shape[-1]))\n",
        "  counter = 0\n",
        "  for i in x_arr:\n",
        "      img = mtf.fit_transform(np.transpose(i.reshape(-1,1)))\n",
        "      x_train_mtf_images[counter] = img[0]\n",
        "      counter = counter + 1\n",
        "\n",
        "  return x_train_mtf_images\n",
        "\n",
        "def make_image(x_arr):\n",
        "\n",
        "  gadf_img = make_gasf_image(x_arr)\n",
        "  gasf_img = make_gadf_image(x_arr)\n",
        "  mtf_img  = apply_markov_transition(x_arr)\n",
        "\n",
        "  img = np.concatenate((np.expand_dims(gasf_img, axis=3),\n",
        "                        np.expand_dims(gadf_img, axis=3),\n",
        "                        np.expand_dims(mtf_img, axis=3)), axis=3)\n",
        "\n",
        "  return img"
      ],
      "id": "2bnDyWyngOnq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixi60_qHS8vY"
      },
      "source": [
        "**TRAIN DIFFERENT MODELS**"
      ],
      "id": "ixi60_qHS8vY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdGHNjtpJSO-"
      },
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "\n",
        "tgt = 'cholesterol_ldl_human'\n",
        "\n",
        "pred_ldl = train_model_NN(df_ldl_f, df_test_ldl, n_folds = 10)"
      ],
      "id": "DdGHNjtpJSO-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGOMbNR6K0uG"
      },
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "\n",
        "tgt = 'hdl_cholesterol_human'\n",
        "\n",
        "pred_hdl = train_model_NN(df_hdl_f, df_test_hdl, n_folds = 10)"
      ],
      "id": "FGOMbNR6K0uG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ht3SJI0NFun"
      },
      "outputs": [],
      "source": [
        "seed_everything(42)\n",
        "\n",
        "tgt = 'hemoglobin(hgb)_human'\n",
        "\n",
        "pred_hgb = train_model_NN(df_hgb_f, df_test_hgb, n_folds = 10)"
      ],
      "id": "0Ht3SJI0NFun"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb6-Hj2ZRUxf"
      },
      "outputs": [],
      "source": [
        "pred_hdl_ = gmean(pred_hdl, 0)\n",
        "pred_ldl_ = gmean(pred_ldl, 0)\n",
        "pred_hgb_ = gmean(pred_hgb, 0)"
      ],
      "id": "Kb6-Hj2ZRUxf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCD8tUIcj7bV"
      },
      "outputs": [],
      "source": [
        "predictions = np.zeros((test_.shape[0], len(targets)))\n",
        "predictions[:, 0] = pred_hdl_.argmax(-1)\n",
        "predictions[:, 1] = pred_ldl_.argmax(-1)\n",
        "predictions[:, 2] = pred_hgb_.argmax(-1)"
      ],
      "id": "DCD8tUIcj7bV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAKU8nsyKabo"
      },
      "outputs": [],
      "source": [
        "def transform(values):\n",
        "  labels = []\n",
        "\n",
        "  for value in values:\n",
        "    if value == 0:\n",
        "        labels.append('low')\n",
        "    elif value == 1:\n",
        "        labels.append('ok')\n",
        "    elif value == 2:\n",
        "        labels.append('high')\n",
        "\n",
        "  return labels"
      ],
      "id": "hAKU8nsyKabo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ullgrXc4ZdY4"
      },
      "outputs": [],
      "source": [
        "df_test_hdl[targets[0]] = predictions[:, 0]\n",
        "df_test_ldl[targets[1]] = predictions[:, 1]\n",
        "df_test_hgb[targets[2]] = predictions[:, 2]"
      ],
      "id": "ullgrXc4ZdY4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B906i6WSapBf"
      },
      "outputs": [],
      "source": [
        "sub_hdl = df_test_hdl[['donation_id', targets[0]]].copy()\n",
        "sub_ldl = df_test_ldl[['donation_id', targets[1]]].copy()\n",
        "sub_hgb = df_test_hgb[['donation_id', targets[2]]].copy()"
      ],
      "id": "B906i6WSapBf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMET4XozKLRz"
      },
      "outputs": [],
      "source": [
        "sub_df = sub_hdl.merge(sub_ldl, on = 'donation_id', how = 'left')\n",
        "sub_df = sub_df.merge(sub_hgb, on = 'donation_id', how = 'left')"
      ],
      "id": "pMET4XozKLRz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBFWto4wPGqH"
      },
      "outputs": [],
      "source": [
        "sub_df[targets] = sub_df[targets].astype(int)"
      ],
      "id": "GBFWto4wPGqH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_ihfz6KKkYy"
      },
      "outputs": [],
      "source": [
        "sub_df[targets] = sub_df[targets].apply(lambda x: transform(x.values), axis=0) "
      ],
      "id": "0_ihfz6KKkYy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUq7RCl-Tc66"
      },
      "outputs": [],
      "source": [
        "final1_class_map = dict(zip(sub_df['donation_id'], sub_df[targets[0]]))\n",
        "final2_class_map = dict(zip(sub_df['donation_id'], sub_df[targets[1]]))\n",
        "final3_class_map = dict(zip(sub_df['donation_id'], sub_df[targets[2]]))"
      ],
      "id": "LUq7RCl-Tc66"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrSQBk0_UL4G"
      },
      "outputs": [],
      "source": [
        "test[targets[0]] = test['donation_id'].map(final1_class_map)\n",
        "test[targets[1]] = test['donation_id'].map(final2_class_map)\n",
        "test[targets[2]] = test['donation_id'].map(final3_class_map)"
      ],
      "id": "MrSQBk0_UL4G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1dXI4x_UrWq"
      },
      "outputs": [],
      "source": [
        "test_sub = test.copy()"
      ],
      "id": "w1dXI4x_UrWq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQXXO_ymssxj"
      },
      "outputs": [],
      "source": [
        "test_sub.shape"
      ],
      "id": "CQXXO_ymssxj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnFZajQ943Ed"
      },
      "outputs": [],
      "source": [
        "test_sub"
      ],
      "id": "XnFZajQ943Ed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uj7iY8opVi8q"
      },
      "outputs": [],
      "source": [
        "test_sub[targets] = test_sub[targets].astype(str) "
      ],
      "id": "Uj7iY8opVi8q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-1MrkxTQYFi"
      },
      "outputs": [],
      "source": [
        "test_sub['donation_id'] = test_sub['donation_id'].astype(str)"
      ],
      "id": "M-1MrkxTQYFi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzFm-oiMRzx0"
      },
      "outputs": [],
      "source": [
        "hdl_rows = pd.DataFrame(test_sub[['donation_id'] +  targets].apply(transform_c_hdl, axis=1))\n",
        "hemo_rows = pd.DataFrame(test_sub[['donation_id'] + targets].apply(transform_hemo, axis=1))\n",
        "ldl_rows = pd.DataFrame(test_sub[['donation_id'] +  targets].apply(transform_c_ldl, axis=1))"
      ],
      "id": "QzFm-oiMRzx0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY_IZeYRTApN"
      },
      "outputs": [],
      "source": [
        "ss = pd.concat([hdl_rows, hemo_rows, ldl_rows]).reset_index(drop=True)"
      ],
      "id": "mY_IZeYRTApN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCTpsHrKn9lJ"
      },
      "outputs": [],
      "source": [
        "ss[\"target\"] = ss[0].apply(lambda x: x.split(\"-\")[1])\n",
        "ss[0] = ss[0].apply(lambda x: x.split(\"-\")[0])"
      ],
      "id": "oCTpsHrKn9lJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6H22NOgSJA9"
      },
      "outputs": [],
      "source": [
        "ss = ss.rename(columns={0:\"Donation_ID\"})"
      ],
      "id": "X6H22NOgSJA9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-O3CvWTLP6i"
      },
      "outputs": [],
      "source": [
        "ss['Donation_ID'] = 'ID_' + ss['Donation_ID']\n",
        "ss"
      ],
      "id": "o-O3CvWTLP6i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTae7gzn8Yzt"
      },
      "outputs": [],
      "source": [
        "ss.target.value_counts()"
      ],
      "id": "uTae7gzn8Yzt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVHG_hACLUko"
      },
      "outputs": [],
      "source": [
        "ss.to_csv(\"submission.csv\", index=False)"
      ],
      "id": "fVHG_hACLUko"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iZw6hkjTV-m9"
      },
      "id": "iZw6hkjTV-m9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Team_blood_metabolites_solution.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}